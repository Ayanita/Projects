{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**New York City Taxi Ride Duration Prediction**\n",
    "\n",
    "In this case study, we will build a predictive model to predict the duration of taxi ride. We will do the following steps:\n",
    "  * Install the dependencies\n",
    "  * Load the data as pandas dataframe\n",
    "  * Define the outcome variable - the variable we are trying to predict.\n",
    "  * Build features with Deep Feature Synthesis using the [featuretools](https://featuretools.com) package. We will start with simple features and incrementally improve the feature definitions and examine the accuracy of the system.\n",
    "  \n",
    "\n",
    "\n",
    "Allocate at least 2-3 hours to go through this case study end-to-end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Dependencies**\n",
    "<p>If you have not done so already, download this repository <a href=\"https://github.com/Featuretools/DSx/archive/master.zip\">from git</a>. Once you have downloaded this archive, unzip it and cd into the directory from the command line. Next run the command ``./install_osx.sh`` if you are on a mac or ``./install_linux.sh`` if you are on linux. This should install all of the dependencies.</p>\n",
    "<p> If you are on a windows machine, open the requirements.txt folder and make sure to install each of the dependencies listed (featuretools, jupyter, pandas, sklearn, numpy) </p>\n",
    "<p> Once you have installed all of the dependencies, open this notebook. On Mac and Linux, navigate to the directory that you downloaded from git and run ``jupyter notebook`` to be taken to this notebook in your default web browser. When you open the NewYorkCity_taxi_case_study.ipynb file in the web browser, you can step through the code by clicking the ``Run`` button at the top of the page. If you have any questions for how to use <a href=\"http://jupyter.org/\">Jupyter</a>, refer to google or the discussion forum.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T12:40:11.695481Z",
     "start_time": "2021-07-15T12:38:57.408993Z"
    }
   },
   "outputs": [],
   "source": [
    "import featuretools as ft\n",
    "import utils\n",
    "from utils import load_nyc_taxi_data, compute_features, preview, feature_importances\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from featuretools.primitives import (Minute, Hour, Day, Week, Month,\n",
    "                                     Weekday, IsWeekend, Count, Sum, Mean, Median, Std, Min, Max)\n",
    "import numpy as np\n",
    "ft.__version__\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Download and load the raw data as pandas dataframes**\n",
    "<p>If you have not yet downloaded the data it can be downloaded <a href=\"https://s3.amazonaws.com/mit-dsx-data/nyc-taxi-data.zip\">from S3</a>. Once you have downloaded the archive, unzip it and place the nyc-taxi-data folder in the same directory as this script. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T12:40:47.210990Z",
     "start_time": "2021-07-15T12:40:41.965643Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trips, pickup_neighborhoods, dropoff_neighborhoods = load_nyc_taxi_data()\n",
    "preview(trips, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``trips`` table has the following fields\n",
    "* ``id`` which uniquely identifies the trip\n",
    "* ``vendor_id`` is the taxi cab company - in our case study we have data from three different cab companies\n",
    "* ``pickup_datetime`` the time stamp for pickup\n",
    "* ``dropoff_datetime`` the time stamp for drop-off\n",
    "* ``passenger_count`` the number of passengers for the trip\n",
    "* ``trip_distance`` total distance of the trip in miles \n",
    "* ``pickup_longitude`` the longitude for pickup\n",
    "* ``pickup_latitude`` the latitude for pickup\n",
    "* ``dropoff_longitude``the longitude of dropoff \n",
    "* ``dropoff_latitude`` the latitude of dropoff\n",
    "* ``payment_type`` a numeric code signifying how the passenger paid for the trip. 1= Credit card 2= Cash 3= No charge 4= Dispute 5= Unknown 6= Voided\n",
    "* ``trip_duration`` this is the duration we would like to predict using other fields \n",
    "* ``pickup_neighborhood`` a one or two letter id of the neighboorhood where the trip started\n",
    "* ``dropoff_neighborhood`` a one or two letter id of the neighboorhood where the trip ended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Prepare the Data**\n",
    "\n",
    "\n",
    "Lets create entities and relationships. The three entities in this data are \n",
    "* trips \n",
    "* pickup_neighborhoods\n",
    "* dropoff_neighborhoods\n",
    "\n",
    "This data has the following relationships\n",
    "* pickup_neighborhoods --> trips (one neighboorhood can have multiple trips that start in it. This means pickup_neighborhoods is the ``parent_entity`` and trips is the child entity)\n",
    "* dropoff_neighborhoods --> trips (one neighboorhood can have multiple trips that end in it. This means dropoff_neighborhoods is the ``parent_entity`` and trips is the child entity)\n",
    "\n",
    "In <a <href=\"https://www.featuretools.com/\"><featuretools (automated feature engineering software package)/></a>, we specify the list of entities and relationships as follows: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T12:41:08.351728Z",
     "start_time": "2021-07-15T12:41:08.253155Z"
    }
   },
   "outputs": [],
   "source": [
    "entities = {\n",
    "        \"trips\": (trips, \"id\", 'pickup_datetime' ),\n",
    "        \"pickup_neighborhoods\": (pickup_neighborhoods, \"neighborhood_id\"),\n",
    "        \"dropoff_neighborhoods\": (dropoff_neighborhoods, \"neighborhood_id\"),\n",
    "        }\n",
    "\n",
    "relationships = [(\"pickup_neighborhoods\", \"neighborhood_id\", \"trips\", \"pickup_neighborhood\"),\n",
    "                 (\"dropoff_neighborhoods\", \"neighborhood_id\", \"trips\", \"dropoff_neighborhood\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify the cutoff time for each instance of the target_entity, in this case ``trips``.This timestamp represents the last time data can be used for calculating features by DFS. In this scenario, that would be the pickup time because we would like to make the duration prediction using data before the trip starts. \n",
    "\n",
    "For the purposes of the case study, we choose to only select trips that started after January 12th, 2016. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1.1 and 1.2\n",
    "\n",
    "**1.1 Paste in the first 5 Cutoff Times**\n",
    "\n",
    "**1.2 Do these cutoff times make sense? Why did we choose these for the cutoff times?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T12:41:53.295477Z",
     "start_time": "2021-07-15T12:41:53.056658Z"
    }
   },
   "outputs": [],
   "source": [
    "cutoff_time = trips[['id', 'pickup_datetime']]\n",
    "cutoff_time = cutoff_time[cutoff_time['pickup_datetime'] > \"2016-01-12\"]\n",
    "preview(cutoff_time, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Create baseline features using Deep Feature Synthesis**\n",
    "\n",
    "Instead of manually creating features, such as \"month of pickup datetime\", we can let DFS come up with them automatically. It does this by \n",
    "* interpreting the variable types of the columns e.g categorical, numeric and others \n",
    "* matching the columns to the primitives that can be applied to their variable types\n",
    "* creating features based on these matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create transform features using transform primitives**\n",
    "\n",
    "As we described in the video, features fall into two major categories, ``transform`` and ``aggregate``. In featureools, we can create transform features by specifying ``transform`` primitives. Below we specify a ``transform`` primitive called ``weekend`` and here is what it does:\n",
    "\n",
    "* It can be applied to any ``datetime`` column in the data. \n",
    "* For each entry in the column, it assess if it is a ``weekend`` and returns a boolean. \n",
    "\n",
    "In this specific data, there are two ``datetime`` columns ``pickup_datetime`` and ``dropoff_datetime``. The tool automatically creates features using the primitive and these two columns as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T12:46:18.552030Z",
     "start_time": "2021-07-15T12:46:16.349739Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_primitives = [IsWeekend]\n",
    "\n",
    "features = ft.dfs(entities=entities,\n",
    "                  relationships=relationships,\n",
    "                  target_entity=\"trips\",\n",
    "                  trans_primitives=trans_primitives,\n",
    "                  agg_primitives=[],\n",
    "                  ignore_variables={\"trips\": [\"pickup_latitude\", \"pickup_longitude\",\n",
    "                                              \"dropoff_latitude\", \"dropoff_longitude\"]},\n",
    "                  features_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you're interested about parameters to DFS such as `ignore_variables`, you can learn more about these parameters [here](https://docs.featuretools.com/generated/featuretools.dfs.html#featuretools.dfs)*\n",
    "<p>Here are the features created.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T12:46:24.799450Z",
     "start_time": "2021-07-15T12:46:24.699552Z"
    }
   },
   "outputs": [],
   "source": [
    "print (\"Number of features: %d\" % len(features))\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T12:46:56.132642Z",
     "start_time": "2021-07-15T12:46:56.030342Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_features(features, cutoff_time):\n",
    "    # shuffle so we don't see encoded features in the front or backs\n",
    "\n",
    "    np.random.shuffle(features)\n",
    "    feature_matrix = ft.calculate_feature_matrix(features,\n",
    "                                                 cutoff_time=cutoff_time,\n",
    "                                                 approximate='36d',\n",
    "                                                 verbose=True,entities=entities, relationships=relationships)\n",
    "    print(\"Finishing computing...\")\n",
    "    feature_matrix, features = ft.encode_features(feature_matrix, features,\n",
    "                                                  to_encode=[\"pickup_neighborhood\", \"dropoff_neighborhood\"],\n",
    "                                                  include_unknown=False)\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T12:47:08.688014Z",
     "start_time": "2021-07-15T12:46:57.998218Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_matrix = compute_features(features, cutoff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T12:47:21.290315Z",
     "start_time": "2021-07-15T12:47:20.995105Z"
    }
   },
   "outputs": [],
   "source": [
    "preview(feature_matrix, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build the Model**\n",
    "\n",
    "# Question 2.1 and 2.2\n",
    "\n",
    "**2.1 What was the Modeling Score after your last training round?**\n",
    "\n",
    "**2.2 Hypothesize on how including more robust features will change the accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a model, we\n",
    "* Seperate the data into a porition for ``training`` (75% in this case) and a portion for ``testing`` \n",
    "* Get the log of the trip duration so that a more linear relationship can be found.\n",
    "* Train a model using a ``GradientBoostingRegressor``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T12:47:30.919229Z",
     "start_time": "2021-07-15T12:47:24.754592Z"
    }
   },
   "outputs": [],
   "source": [
    "# separates the whole feature matrix into train data feature matrix, \n",
    "# train data labels, and test data feature matrix \n",
    "X_train, y_train, X_test, y_test = utils.get_train_test_fm(feature_matrix,.75)\n",
    "y_train = np.log(y_train+1)\n",
    "y_test = np.log(y_test+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T12:49:36.193787Z",
     "start_time": "2021-07-15T12:47:34.738172Z"
    }
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(verbose=True)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3.1 and 3.2\n",
    "\n",
    "**3.1 Paste in 5 of the  generated features**\n",
    "\n",
    "**3.2 What are these features called and how are they generated?**\n",
    "\n",
    "**Step 5: Adding more Transform Primitives**\n",
    "\n",
    "* Add ``Minute``, ``Hour``, ``Week``, ``Month``, ``Weekday`` , etc primitives\n",
    "* All these transform primitives apply to ``datetime`` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T12:51:54.063760Z",
     "start_time": "2021-07-15T12:51:51.694558Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_primitives = [Minute, Hour, Day, Week, Month, Weekday, IsWeekend]\n",
    "\n",
    "features = ft.dfs(entities=entities,\n",
    "                  relationships=relationships,\n",
    "                  target_entity=\"trips\",\n",
    "                  trans_primitives=trans_primitives,\n",
    "                  agg_primitives=[],\n",
    "                  ignore_variables={\"trips\": [\"pickup_latitude\", \"pickup_longitude\",\n",
    "                                              \"dropoff_latitude\", \"dropoff_longitude\"]},\n",
    "                  features_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T12:52:31.456696Z",
     "start_time": "2021-07-15T12:52:31.362945Z"
    }
   },
   "outputs": [],
   "source": [
    "print (\"Number of features: %d\" % len(features))\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T12:52:48.471903Z",
     "start_time": "2021-07-15T12:52:34.218475Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_matrix = compute_features(features, cutoff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T12:52:50.630760Z",
     "start_time": "2021-07-15T12:52:50.137051Z"
    }
   },
   "outputs": [],
   "source": [
    "preview(feature_matrix, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4.1 and 4.2\n",
    "\n",
    "**4.1 What was the Modeling Score after your last training round when including the transform primitives?**\n",
    "\n",
    "**4.2 Comment on how the modeling accuracy differs when including the Transform features.**\n",
    "\n",
    "\n",
    "**Step 6: Build the new model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T12:55:38.459145Z",
     "start_time": "2021-07-15T12:55:32.592515Z"
    }
   },
   "outputs": [],
   "source": [
    "# separates the whole feature matrix into train data feature matrix,\n",
    "# train data labels, and test data feature matrix \n",
    "X_train, y_train, X_test, y_test = utils.get_train_test_fm(feature_matrix,.75)\n",
    "y_train = np.log(y_train+1)\n",
    "y_test = np.log(y_test+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T12:58:54.919747Z",
     "start_time": "2021-07-15T12:55:40.250447Z"
    }
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(verbose=True)\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7: Add Aggregation Primitives**\n",
    "\n",
    "Now let's add aggregation primitives. These primitives will generate features for the parent entities ``pickup_neighborhoods``, and ``dropoff_neighborhood`` and then add them to the trips entity, which is the entity for which we are trying to make prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T12:59:43.977607Z",
     "start_time": "2021-07-15T12:59:41.353328Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_primitives = [Minute, Hour, Day, Week, Month, Weekday, IsWeekend]\n",
    "aggregation_primitives = [Count, Sum, Mean, Median, Std, Max, Min]\n",
    "\n",
    "features = ft.dfs(entities=entities,\n",
    "                  relationships=relationships,\n",
    "                  target_entity=\"trips\",\n",
    "                  trans_primitives=trans_primitives,\n",
    "                  agg_primitives=aggregation_primitives,\n",
    "                  ignore_variables={\"trips\": [\"pickup_latitude\", \"pickup_longitude\",\n",
    "                                              \"dropoff_latitude\", \"dropoff_longitude\"]},\n",
    "                  features_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T12:59:45.888070Z",
     "start_time": "2021-07-15T12:59:45.782499Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print (\"Number of features: %d\" % len(features))\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T11:13:54.617340Z",
     "start_time": "2021-07-09T11:13:28.226824Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_matrix = compute_features(features, cutoff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T11:14:02.858716Z",
     "start_time": "2021-07-09T11:14:02.439924Z"
    }
   },
   "outputs": [],
   "source": [
    "preview(feature_matrix, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5.1 and 5.2\n",
    "\n",
    "**5.1 What was the Modeling Score after your last training round when including the aggregate transforms?**\n",
    "\n",
    "**5.2 How do these aggregate transforms impact performance? How do they impact training time?**\n",
    "\n",
    "**Step 8: Build the new model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T11:14:41.627956Z",
     "start_time": "2021-07-09T11:14:07.052570Z"
    }
   },
   "outputs": [],
   "source": [
    "# separates the whole feature matrix into train data feature matrix,\n",
    "# train data labels, and test data feature matrix \n",
    "X_train, y_train, X_test, y_test = utils.get_train_test_fm(feature_matrix,.75)\n",
    "y_train = np.log(y_train+1)\n",
    "y_test = np.log(y_test+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T11:22:55.037382Z",
     "start_time": "2021-07-09T11:14:42.855859Z"
    }
   },
   "outputs": [],
   "source": [
    "# note: this may take up to 30 minutes to run\n",
    "model = GradientBoostingRegressor(verbose=True)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Evalute on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T11:22:56.463613Z",
     "start_time": "2021-07-09T11:22:55.656703Z"
    }
   },
   "outputs": [],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also make predictions using our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T11:23:07.091360Z",
     "start_time": "2021-07-09T11:23:06.179570Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.exp(y_pred) - 1 # undo the log we took earlier\n",
    "y_pred[5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Analysis\n",
    "<p>Let's look at how important each feature was for the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T11:23:15.953274Z",
     "start_time": "2021-07-09T11:23:15.813531Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_importances(model, feature_matrix.columns, n=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
